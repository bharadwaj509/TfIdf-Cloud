import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class TfIdf2 {

	   public static class Map2 extends Mapper<LongWritable, Text, Text, Text> {
		 
		   /**
		     * @param key is the byte offset of the current line in the file;
		     * @param value is the line from the file
		     * @param context
		     *
		     *     PRE-CONDITION: aa@leornardo-davinci-all.txt    1
		     *                    aaron@all-shakespeare   98
		     *                    ab@leornardo-davinci-all.txt    3
		     *
		     *     POST-CONDITION: Output <"all-shakespeare", "aaron=98"> pairs
		     */
		    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		        String[] wordAndDocCounter = value.toString().split("\t");
		        String[] wordAndDoc = wordAndDocCounter[0].split("@");
		        context.write(new Text(wordAndDoc[1]), new Text(wordAndDoc[0] + "=" + wordAndDocCounter[1]));
		    }
		}
		
		public static class Reduce2 extends Reducer<Text, Text, Text, Text> {
		 
		    /**
		     * @param key is the key of the mapper
		     * @param values are all the values aggregated during the mapping phase
		     * @param context contains the context of the job run
		     *
		     *        PRE-CONDITION: receive a list of <document, ["word=n", "word-b=x"]>
		     *            pairs <"a.txt", ["word1=3", "word2=5", "word3=5"]>
		     *
		     *       POST-CONDITION: <"word1@a.txt, 3/13">,
		     *            <"word2@a.txt, 5/13">
		     */
		    protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
		        int sumOfWordsInDocument = 0;
		        Map<String, Integer> tempCounter = new HashMap<String, Integer>();
		        for (Text val : values) {
		            String[] wordCounter = val.toString().split("=");
		            tempCounter.put(wordCounter[0], Integer.valueOf(wordCounter[1]));
		            sumOfWordsInDocument += Integer.parseInt(val.toString().split("=")[1]);
		        }
		        for (String wordKey : tempCounter.keySet()) {
		            context.write(new Text(wordKey + "@" + key.toString()), new Text(tempCounter.get(wordKey) + "/"
		                    + sumOfWordsInDocument));
		        }
		    }
		}
		
	
	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException {
		// TODO Auto-generated method stub
 
		/**
		 * WordCountsInDocuments counts the total number of words in each document and
		 * produces data with the relative and total number of words for each document.
		 * Hadoop 0.20.1 API
		 * @author Marcello de Sales (marcello.desales@gmail.com)
		 */
		Configuration conf = new Configuration();
		@SuppressWarnings("deprecation")
		Job job = new Job(conf, "TfIdf Job 2");
		 
		job.setJarByClass(TfIdf2.class);
		job.setMapperClass(Map2.class);
		job.setReducerClass(Reduce2.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
        job.setInputFormatClass(KeyValueTextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[0]));
		
		 
		job.waitForCompletion(true);
		
	}

}
